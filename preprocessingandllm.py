# -*- coding: utf-8 -*-
"""PreprocessingandLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17kBrjDA7wSHiktmdiwsv7idcNJOEL3TB

# INSTALLAZIONE LIBRERIE
"""

!pip install spacy
!python -m spacy download en_core_web_sm
!python -m spacy download it_core_news_sm
!pip install translate-toolki
!pip install keras
!pip install tensorflow
!pip install translate-toolkit

from google.colab import drive
from translate.storage import tmx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import spacy
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
import tensorflow as tf
import keras
from keras.preprocessing.sequence import pad_sequences
import sklearn
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time

"""# DATASET"""

drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/en-it.tmx"
sentence_pairs = []

# Open the file in read mode
with open(file_path, 'rb') as f:
  tmx_file = tmx.tmxfile(f)

# Iterate through the translation units and store sentence pairs
for unit in tmx_file.units:
    source_text = unit.source
    target_text = unit.target
    sentence_pairs.append((source_text, target_text))

# Create a DataFrame from the sentence pairs
df = pd.DataFrame(sentence_pairs, columns=['Source', 'Target']);

"""# PREPROCESSING"""

# Funzione di pulizia per rimuovere caratteri speciali e gestire gli accenti
def clean_text(text):
    text = re.sub(r"http\S+|www.\S+", "", text)
    text = re.sub(r"[^a-zA-ZàèéìòùÀÈÉÌÒÙçÇ]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# Applichiamo la pulizia su Source e Target
df['Source_clean'] = df['Source'].apply(clean_text)
df['Target_clean'] = df['Target'].apply(clean_text)

# Rimuove le righe con frasi vuote
df = df[df['Source_clean'].str.strip() != '']
df = df[df['Target_clean'].str.strip() != '']

# Rimuove frasi troppo corte (esempio: meno di 3 parole)
df = df[df['Source_clean'].apply(lambda x: len(x.split()) >= 3)]
df = df[df['Target_clean'].apply(lambda x: len(x.split()) >= 3)]

# Verifica la presenza di eventuali valori nulli
print(df.isnull().sum())

# Rimozione duplicati
df.drop_duplicates(subset=['Source_clean', 'Target_clean'], inplace=True)

# Verifica dei risultati puliti
df[['Source_clean', 'Target_clean']].head(10)

# Rimuove frasi troppo lunghe (più di 96 parole)
df = df[df['Source'].apply(lambda x: len(x.split()) <= 96)]
df = df[df['Target'].apply(lambda x: len(x.split()) <= 96)]
# Stampa la lunghezza del dataset
print(f"Dataset length after the removal of too long sentences: {len(df)}")

from transformers import MarianTokenizer
from sklearn.model_selection import train_test_split
import torch

# Initialize tokenizer
# tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-it')

# Step 1: Split the data before tokenizing to keep references to the original sentences
#train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)

# Step 2: Tokenize each split independently to avoid handling tensor splits
#train_source_tokens = tokenizer(train_df['Source_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
#train_target_tokens = tokenizer(train_df['Target_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
#test_source_tokens = tokenizer(test_df['Source_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
#test_target_tokens = tokenizer(test_df['Target_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)

# Step 3: Assign tokenized IDs and attention masks for both source and target languages
#train_source_ids, train_source_mask = train_source_tokens['input_ids'], train_source_tokens['attention_mask']
#train_target_ids, train_target_mask = train_target_tokens['input_ids'], train_target_tokens['attention_mask']
#test_source_ids, test_source_mask = test_source_tokens['input_ids'], test_source_tokens['attention_mask']
#test_target_ids, test_target_mask = test_target_tokens['input_ids'], test_target_tokens['attention_mask']

# Step 4: Move tensors to GPU if available
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#train_source_ids, test_source_ids = train_source_ids.to(device), test_source_ids.to(device)
#train_source_mask, test_source_mask = train_source_mask.to(device), test_source_mask.to(device)
#train_target_ids, test_target_ids = train_target_ids.to(device), test_target_ids.to(device)
#train_target_mask, test_target_mask = train_target_mask.to(device), test_target_mask.to(device)

"""# CAMPIONAMENTO DATASET"""

# Campionamento casuale del 5% del dataset
sample_size = int(0.01 * len(df))  # Modifica a 0.10 per il 10%
sampled_data = df.sample(n=sample_size, random_state=42).reset_index(drop=True)

"""# MODELLO LLM"""

from transformers import MarianMTModel, MarianTokenizer

model_name = 'Helsinki-NLP/opus-mt-en-it'
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
# Spostare il modello sul device corretto
# model.to(device)

"""# TRADUZIONE FRASI"""

# Funzione per tradurre le frasi
def translate_texts(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    translated = model.generate(**inputs)
    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]

# Traduci solo il sottoinsieme campionato
sampled_data['translated'] = translate_texts(sampled_data['Source_clean'].tolist())

"""# VALUTAZIONE"""

import sacrebleu

# Valutazione BLEU sul campione
bleu = sacrebleu.corpus_bleu(sampled_data['translated'], [sampled_data['target_clean']])
print(f"BLEU score for sample: {bleu.score}")



"""# FINE-TUNING"""