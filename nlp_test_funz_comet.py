# -*- coding: utf-8 -*-
"""NLP_Test_Funz_comet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQZyRuNIF2xMr0xoa8mBnb41TZbFqWZx

# **LIBRARIES AND DATA**
"""

!pip install tqdm
!pip install keras
!pip install tensorflow
!pip install translate-toolkit
!pip install evaluate
!pip install nltk rouge-score
!pip install unbabel-comet;

from google.colab import drive
from translate.storage import tmx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import spacy
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from wordcloud import WordCloud
import tensorflow as tf
import keras
from keras.preprocessing.sequence import pad_sequences
import sklearn
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from tqdm import tqdm
from evaluate import load
import xml.etree.ElementTree as ET;

import pytorch_lightning as pl
import logging

logging.getLogger("pytorch_lightning").setLevel(logging.ERROR)  # Mostra solo messaggi di errore

# Monta Google Drive per caricare il file
drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/en-it.tmx"
sentence_pairs = []

# Open the file in read mode
with open(file_path, 'rb') as f:
  tmx_file = tmx.tmxfile(f)

# Iterate through the translation units and store sentence pairs
for unit in tmx_file.units:
    source_text = unit.source
    target_text = unit.target
    sentence_pairs.append((source_text, target_text))

# Create a DataFrame from the sentence pairs
df = pd.DataFrame(sentence_pairs, columns=['Source', 'Target']);

"""# **PRE - PROCESSING**"""

# Funzione di pulizia per rimuovere caratteri speciali e gestire gli accenti
def clean_text(text):
    text = re.sub(r"http\S+|www.\S+", "", text)
    text = re.sub(r"[^a-zA-ZàèéìòùÀÈÉÌÒÙçÇ]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# Applichiamo la pulizia su Source e Target
df['Source_clean'] = df['Source'].apply(clean_text)
df['Target_clean'] = df['Target'].apply(clean_text)

# Rimuove le righe con frasi vuote
df = df[df['Source_clean'].str.strip() != '']
df = df[df['Target_clean'].str.strip() != '']

# Rimuove frasi troppo corte (esempio: meno di 3 parole)
df = df[df['Source_clean'].apply(lambda x: len(x.split()) >= 3)]
df = df[df['Target_clean'].apply(lambda x: len(x.split()) >= 3)]

# Verifica la presenza di eventuali valori nulli
print(df.isnull().sum())

# Rimozione duplicati
df.drop_duplicates(subset=['Source_clean', 'Target_clean'], inplace=True)

# Verifica dei risultati puliti
df[['Source_clean', 'Target_clean']].head(10)

"""# **EDA**

## *Sentence length distribution*
"""

# Calcola la lunghezza delle frasi
df['source_length'] = df['Source'].apply(lambda x: len(x.split()))
df['target_length'] = df['Target'].apply(lambda x: len(x.split()))

# Plot della distribuzione delle lunghezze
plt.figure(figsize=(10, 6))
plt.hist(df['source_length'], bins = 100, alpha=0.7, label='Source', color = "blue")
plt.hist(df['target_length'], bins = 100, alpha=0.7, label='Target', color =  "orange")
plt.legend(loc='upper right')
plt.title('Sentence length distribution')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xlim(0, 150)
plt.show()

"""## *Most frequent words*"""

# Funzione per ottenere le parole più comuni
def plot_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, size=15)
    plt.axis('off')
    plt.show()

# Parole più comuni nelle frasi di Source
source_words = df['Source'].str.split().explode()
plot_wordcloud(source_words, 'Most common words in english')

# Parole più comuni nelle frasi di Target
target_words = df['Target'].str.split().explode()
plot_wordcloud(target_words, 'Most common words in italian')

"""## *Source - Target Words Ratio*"""

plt.figure(figsize=(10, 6))
plt.scatter(df['source_length'], df['target_length'], alpha=0.5)
plt.title('Source - Target Words Ratio')
plt.xlabel('Source Length')
plt.ylabel('Target Length')
plt.xlim(0, 150)
plt.ylim(0, 150)
plt.show()

"""### Removing too long sentences"""

# Rimuove frasi troppo lunghe (più di 96 parole)
df = df[df['Source'].apply(lambda x: len(x.split()) <= 96)]
df = df[df['Target'].apply(lambda x: len(x.split()) <= 96)]
# Stampa la lunghezza del dataset
print(f"Dataset length after the removal of too long sentences: {len(df)}")

"""# **TOKENIZATION**"""

from transformers import MarianTokenizer
from sklearn.model_selection import train_test_split
import torch

# Initialize tokenizer
tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-it')

# Step 1: Split the data before tokenizing to keep references to the original sentences
train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)

# Step 2: Tokenize each split independently to avoid handling tensor splits
train_source_tokens = tokenizer(train_df['Source_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
train_target_tokens = tokenizer(train_df['Target_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
test_source_tokens = tokenizer(test_df['Source_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)
test_target_tokens = tokenizer(test_df['Target_clean'].tolist(), return_tensors="pt", padding=True, truncation=True, max_length=96)

# Step 3: Assign tokenized IDs and attention masks for both source and target languages
train_source_ids, train_source_mask = train_source_tokens['input_ids'], train_source_tokens['attention_mask']
train_target_ids, train_target_mask = train_target_tokens['input_ids'], train_target_tokens['attention_mask']
test_source_ids, test_source_mask = test_source_tokens['input_ids'], test_source_tokens['attention_mask']
test_target_ids, test_target_mask = test_target_tokens['input_ids'], test_target_tokens['attention_mask']

# Step 4: Move tensors to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_source_ids, test_source_ids = train_source_ids.to(device), test_source_ids.to(device)
train_source_mask, test_source_mask = train_source_mask.to(device), test_source_mask.to(device)
train_target_ids, test_target_ids = train_target_ids.to(device), test_target_ids.to(device)
train_target_mask, test_target_mask = train_target_mask.to(device), test_target_mask.to(device)

"""# **PRE-TRAINED MODEL**"""

from transformers import MarianMTModel

# Carichiamo il modello pre-addestrato
model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-it')

# Spostare il modello sul device corretto
model.to(device)

# Eseguire l'inferenza con torch.no_grad() per risparmiare memoria
model.eval()
batch_size = 32  # Batch size regolabile
translated_sentences = []

# Usa tqdm per tracciare il progresso
with torch.no_grad():
    for i in tqdm(range(0, len(test_source_ids), batch_size), desc="Traduzione in corso"):
        # Estrai il batch corrente dal test set
        batch_input_ids = test_source_ids[i:i + batch_size].to(device)
        batch_attention_mask = test_source_mask[i:i + batch_size].to(device)

        # Genera traduzioni per il batch corrente
        translated_tokens = model.generate(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_mask,
            max_length=96  # Imposta max_length
        )

        # Decodifica i token generati in frasi leggibili
        translated_sentences_batch = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)
        translated_sentences.extend(translated_sentences_batch)

# Decodifica le frasi originali (target) dal test set
test_targets = tokenizer.batch_decode(test_target_ids, skip_special_tokens=True)

# Mostriamo le traduzioni (prendiamo solo le prime 5 frasi per esempio)
for source, target, translation in zip(test_source_ids[:10], test_targets[:10], translated_sentences[:10]):
    decoded_source = tokenizer.decode(source, skip_special_tokens=True)
    print(f"Inglese (Test Set): {decoded_source}")
    print(f"Italiano originale (Test Set): {target}")
    print(f"Italiano tradotto: {translation}")
    print("-" * 50)

"""# **EVALUATION**"""

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer

# Assicurati che NLTK sia configurato
nltk.download('punkt')

# Usa le frasi del test set (come già calcolate nella fase di traduzione)
references = test_targets  # Frasi target dal test set
generated_translations = translated_sentences  # Frasi tradotte dal modello

# Funzione per calcolare BLEU score per ogni coppia di frasi
def calculate_bleu(reference_sentence, translated_sentence):
    # Tokenizza le frasi
    reference_tokens = reference_sentence.split()  # Tokenizza la frase di riferimento
    translated_tokens = translated_sentence.split()  # Tokenizza la frase tradotta
    smoothing_function = SmoothingFunction().method4  # Funzione di smoothing per evitare score di 0
    bleu_score = sentence_bleu([reference_tokens], translated_tokens, smoothing_function=smoothing_function)
    return bleu_score

# Funzione per calcolare ROUGE score
def calculate_rouge(reference_sentence, translated_sentence):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = scorer.score(reference_sentence, translated_sentence)
    return rouge_scores

# Valutazione di tutte le frasi tradotte
bleu_scores = []
rouge_scores = []

for reference, generated in zip(references, generated_translations):
    bleu = calculate_bleu(reference, generated)
    rouge = calculate_rouge(reference, generated)

    bleu_scores.append(bleu)
    rouge_scores.append(rouge)

# Calcolare la media dei BLEU scores
average_bleu_score = sum(bleu_scores) / len(bleu_scores)

# Calcolare la media dei ROUGE scores
average_rouge_1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)
average_rouge_2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)
average_rouge_l = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)

# Visualizzare i risultati
print(f'Average BLEU score: {average_bleu_score:.4f}')
print(f'Average ROUGE-1 F1 score: {average_rouge_1:.4f}')
print(f'Average ROUGE-2 F1 score: {average_rouge_2:.4f}')
print(f'Average ROUGE-L F1 score: {average_rouge_l:.4f}')

"""# **FINE-TUNING**"""

import torch
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import torch.optim as optim

# Imposta il batch size
batch_size = 32

# Creazione del DataLoader per il training
train_dataset = TensorDataset(train_source_ids, train_source_mask, train_target_ids, train_target_mask)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Creazione del DataLoader per il test set
test_dataset = TensorDataset(test_source_ids, test_source_mask, test_target_ids, test_target_mask)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Imposta il modello in modalità di addestramento
model.train()

# Ottimizzatore
optimizer = optim.AdamW(model.parameters(), lr=5e-5)  # Usa AdamW, consigliato per i modelli Transformers

# Imposta il numero di epoche
num_epochs = 2

# Ciclo di addestramento
for epoch in range(num_epochs):
    total_loss = 0
    for step, batch in enumerate(tqdm(train_loader, desc=f"Training Epoch {epoch + 1}")):
        source_ids, source_mask, target_ids, target_mask = batch

        # Sposta i tensori sulla GPU
        source_ids = source_ids.to(device)
        source_mask = source_mask.to(device)
        target_ids = target_ids.to(device)

        # Azzerare i gradienti
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=target_ids)

        # Calcola la perdita
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # Aggiorna i pesi
        optimizer.step()

        # Accumula la perdita totale
        total_loss += loss.item()

        # Stampa ogni 100 step
        if step % 100 == 0:
            print(f"Step [{step}/{len(train_loader)}], Loss: {loss.item():.4f}")

    # Stampa la perdita media per epoca
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch + 1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}")

# Valutazione sul test set
model.eval()  # Imposta il modello in modalità di valutazione
total_test_loss = 0
translated_sentences = []  # Lista per memorizzare le frasi tradotte

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluating"):
        source_ids, source_mask, target_ids, target_mask = batch

        # Sposta i tensori sulla GPU
        source_ids = source_ids.to(device)
        source_mask = source_mask.to(device)
        target_ids = target_ids.to(device)

        # Forward pass
        outputs = model(input_ids=source_ids, attention_mask=source_mask, labels=target_ids)

        # Accumula la perdita
        total_test_loss += outputs.loss.item()

        # Genera traduzioni per il batch corrente
        translated_tokens = model.generate(input_ids=source_ids, attention_mask=source_mask, max_length=96)
        translated_batch = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)
        translated_sentences.extend(translated_batch)  # Aggiungi frasi tradotte alla lista

# Calcola la perdita media del test set
avg_test_loss = total_test_loss / len(test_loader)
print(f"Average Test Loss: {avg_test_loss:.4f}")

"""# **EVALUATION - 2**"""

# Carica la metrica COMET
comet = load("comet")

# Estrai le frasi di riferimento originali e quelle sorgente in inglese dal test set
test_references = test_df['Target_clean'].tolist()  # Frasi originali in italiano
test_sources = test_df['Source_clean'].tolist()  # Frasi originali in inglese

# Verifica che il numero di frasi corrisponda a quelle tradotte
assert len(test_references) == len(translated_sentences), "Mismatch between reference and translated sentences."

# Liste per le metriche
bleu_scores = []
rouge_scores = []
comet_scores = []

# Calcola BLEU, ROUGE e COMET per ogni frase tradotta
for ref, gen, src in zip(test_references, translated_sentences, test_sources):
    # Calcolo BLEU e ROUGE
    bleu = calculate_bleu(ref, gen)
    rouge = calculate_rouge(ref, gen)

    # Calcolo COMET
    comet_result = comet.compute(predictions=[gen], references=[ref], sources=[src])
    comet_score = comet_result["scores"][0]

    # Aggiungi i risultati alle liste
    bleu_scores.append(bleu)
    rouge_scores.append(rouge)
    comet_scores.append(comet_score)

# Stampa un campione di confronti
print("\n--- Sample sentence comparisons ---")
for i in range(min(10, len(test_references))):  # Mostra solo i primi 10 confronti
    print(f"Original Source (English): {test_sources[i]}")
    print(f"Original Target (Italian): {test_references[i]}")
    print(f"Model's Translation: {translated_sentences[i]}")
    print(f"BLEU Score: {round(bleu_scores[i], 3)}")
    print(f"ROUGE Score: {rouge_scores[i]}")
    print(f"COMET Score: {round(comet_scores[i], 3)}\n")

# Calcolo della media delle metriche con arrotondamento a 3 cifre decimali
average_bleu_score = round(sum(bleu_scores) / len(bleu_scores), 3) if bleu_scores else 0.0
average_rouge_1 = round(sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores), 3) if rouge_scores else 0.0
average_rouge_2 = round(sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores), 3) if rouge_scores else 0.0
average_rouge_l = round(sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores), 3) if rouge_scores else 0.0
average_comet_score = round(sum(comet_scores) / len(comet_scores), 3) if comet_scores else 0.0

# Creazione di una tabella per visualizzare i risultati
results_df = pd.DataFrame({
    "Metric": ["BLEU", "ROUGE-1 F1", "ROUGE-2 F1", "ROUGE-L F1", "COMET"],
    "Score": [
        average_bleu_score,
        average_rouge_1,
        average_rouge_2,
        average_rouge_l,
        average_comet_score
    ]
})

# Visualizza la tabella
print("\n--- Average Scores on Test Set ---")
print(results_df.to_markdown(index=False))

# Carica la metrica COMET
comet = load("comet")

# Estrai le frasi di riferimento originali e quelle sorgente in inglese dal test set
test_references = test_df['Target_clean'].tolist()  # Frasi originali in italiano
test_sources = test_df['Source_clean'].tolist()  # Frasi originali in inglese

# Verifica che il numero di frasi corrisponda a quelle tradotte
assert len(test_references) == len(translated_sentences), "Mismatch between reference and translated sentences."

# Inizializza le somme per le medie
sum_bleu, sum_rouge1, sum_rouge2, sum_rougeL, sum_comet = 0, 0, 0, 0, 0

# Calcola BLEU, ROUGE e COMET per ogni frase tradotta e aggiorna le medie in streaming
for idx, (ref, gen, src) in enumerate(zip(test_references, translated_sentences, test_sources)):
    # Calcolo BLEU e ROUGE
    bleu = calculate_bleu(ref, gen)
    rouge = calculate_rouge(ref, gen)

    # Calcolo COMET (in batch ogni 32 frasi per ridurre l'allocazione di memoria)
    if idx % 32 == 0:
        comet_batch = comet.compute(predictions=translated_sentences[idx:idx+32],
                                    references=test_references[idx:idx+32],
                                    sources=test_sources[idx:idx+32])["scores"]
    comet_score = comet_batch[idx % 32]

    # Aggiorna le somme delle metriche
    sum_bleu += bleu
    sum_rouge1 += rouge['rouge1'].fmeasure
    sum_rouge2 += rouge['rouge2'].fmeasure
    sum_rougeL += rouge['rougeL'].fmeasure
    sum_comet += comet_score

    # Stampa un campione di confronti
    if idx < 10:  # Mostra solo i primi 10 confronti
        print(f"Original Source (English): {src}")
        print(f"Original Target (Italian): {ref}")
        print(f"Model's Translation: {gen}")
        print(f"BLEU Score: {round(bleu, 3)}")
        print(f"ROUGE-1 F1: {round(rouge['rouge1'].fmeasure, 3)}")
        print(f"COMET Score: {round(comet_score, 3)}\n")

# Calcolo della media delle metriche
num_sentences = len(test_references)
average_bleu_score = round(sum_bleu / num_sentences, 3) if num_sentences else 0.0
average_rouge_1 = round(sum_rouge1 / num_sentences, 3) if num_sentences else 0.0
average_rouge_2 = round(sum_rouge2 / num_sentences, 3) if num_sentences else 0.0
average_rouge_l = round(sum_rougeL / num_sentences, 3) if num_sentences else 0.0
average_comet_score = round(sum_comet / num_sentences, 3) if num_sentences else 0.0

# Creazione di una tabella per visualizzare i risultati
results_df = pd.DataFrame({
    "Metric": ["BLEU", "ROUGE-1 F1", "ROUGE-2 F1", "ROUGE-L F1", "COMET"],
    "Score": [
        average_bleu_score,
        average_rouge_1,
        average_rouge_2,
        average_rouge_l,
        average_comet_score
    ]
})

# Visualizza la tabella
print("\n--- Average Scores on Test Set ---")
print(results_df.to_markdown(index=False))